{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Recaptioning Task Agent\n",
    "\n",
    "This notebook demonstrates how to use the Encord Agents Task Runner to create a video recaptioning workflow. We'll use the GPT-4o-mini model to automatically generate multiple captions based on a human-written description.\n",
    "\n",
    "The workflow for this agent is as follows:\n",
    "\n",
    "1. A human creates a caption in the first text field of a video\n",
    "2. The agent is triggered and generates three alternative captions\n",
    "3. The human can review and optionally correct the generated captions\n",
    "\n",
    "This notebook will walk through:\n",
    "- Setting up the required dependencies\n",
    "- Understanding the ontology structure needed\n",
    "- Implementing the task agent using the Runner\n",
    "- Running the agent on your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install encord-agents langchain-openai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated\n",
    "\n",
    "import numpy as np\n",
    "from encord.exceptions import LabelRowError\n",
    "from encord.objects.ontology_labels_impl import LabelRowV2\n",
    "from encord.objects.classification_instance import ClassificationInstance\n",
    "from encord.project import Project\n",
    "from encord.workflow.stages.agent import AgentTask\n",
    "from encord_agents.tasks import Runner\n",
    "from encord_agents.tasks.dependencies import dep_single_frame, dep_label_row, Frame\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "\n",
    "You'll need to set up your API keys to use this notebook. Let's configure the OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Your Encord SSH key should be configured in your environment\n",
    "# or you can set it here\n",
    "# os.environ[\"ENCORD_SSH_KEY_FILE\"] = \"/path/to/your_private_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Model\n",
    "\n",
    "Let's define the response model for the agent to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCaptionResponse(BaseModel):\n",
    "    rephrase_1: str \n",
    "    rephrase_2: str     \n",
    "    rephrase_3: str "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "\n",
    "Let's set up the LLM with the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the LLM to follow\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that rephrases captions.\n",
    "\n",
    "I will provide you with a video caption and an image of the scene of the video. \n",
    "\n",
    "The captions follow this format:\n",
    "\n",
    "\"The droid picks up <cup_0> and puts it on the <table_0>.\"\n",
    "\n",
    "The captions that you make should replace the tags, e.g., <cup_0>, with the actual object names.\n",
    "The replacements should be consistent with the scene.\n",
    "\n",
    "Here are three rephrases: \n",
    "\n",
    "1. The droid picks up the blue mug and puts it on the left side of the table.\n",
    "2. The droid picks up the cup and puts it to the left of the plate.\n",
    "3. The droid is picking up the mug on the right side of the table and putting it down next to the plate.\n",
    "\n",
    "You will rephrase the caption in three different ways, as above, the rephrases should be\n",
    "\n",
    "1. Diverse in terms of adjectives, object relations, and object positions.\n",
    "2. Sound in relation to the scene. You cannot talk about objects you cannot see.\n",
    "3. Short and concise. Keep it within one sentence.\n",
    "\"\"\"\n",
    "\n",
    "# Create the LLM instance with structured output\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4)\n",
    "llm_structured = llm.with_structured_output(AgentCaptionResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LLM Prompt Function\n",
    "\n",
    "Now let's create a function to prompt the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gpt(caption: str, image: Frame) -> AgentCaptionResponse:\n",
    "    \"\"\"\n",
    "    Prompt GPT with a caption and an image to get three rephrases of the caption.\n",
    "    \n",
    "    Args:\n",
    "        caption: The original caption written by a human.\n",
    "        image: The frame from the video that the caption refers to.\n",
    "        \n",
    "    Returns:\n",
    "        A structured response with three rephrases of the original caption.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": f\"Video caption: `{caption}`\"},\n",
    "                image.b64_encoding(output_format=\"openai\"),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return llm_structured.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Task Agent\n",
    "\n",
    "Now let's define our task agent function that will be triggered by the Runner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recaption_agent(\n",
    "    project: Project,\n",
    "    label_row: Annotated[LabelRowV2, Annotated],\n",
    "    task: AgentTask,\n",
    "    frame_content: Annotated[np.ndarray, Annotated[dep_single_frame, {\"frame\": 0}]]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Task agent that generates alternative captions for a video based on a human-written caption.\n",
    "    \n",
    "    Args:\n",
    "        project: The Encord project.\n",
    "        label_row: The label row containing the annotations.\n",
    "        task: The agent task.\n",
    "        frame_content: The content of the first frame of the video.\n",
    "        \n",
    "    Returns:\n",
    "        The name of the pathway to follow after completing the task.\n",
    "    \"\"\"\n",
    "    # Get the relevant ontology information\n",
    "    # We expect: [human annotation, llm recaption 1, llm recaption 2, llm recaption 3]\n",
    "    cap, *rs = label_row.ontology_structure.classifications\n",
    "    \n",
    "    # Read the existing human caption\n",
    "    # We'll take the one from the current frame if it exists,\n",
    "    # otherwise the one from frame zero or any caption, in said order.\n",
    "    instances = label_row.get_classification_instances(filter_ontology_classification=cap, filter_frames=[0])\n",
    "    if not instances:\n",
    "        # Nothing to do if there are no human labels\n",
    "        return \"No Human Caption\"\n",
    "    elif len(instances) > 1:\n",
    "        def order_by_current_frame_else_frame_0(instance: ClassificationInstance) -> int:\n",
    "            try:\n",
    "                instance.get_annotation(0)\n",
    "                return 1\n",
    "            except LabelRowError:\n",
    "                return 0\n",
    "\n",
    "        instance = sorted(instances, key=order_by_current_frame_else_frame_0)[-1]\n",
    "    else:\n",
    "        instance = instances[0]\n",
    "\n",
    "    # Read the actual string caption\n",
    "    caption = instance.get_answer()\n",
    "    \n",
    "    # Run the first frame of the video and the human caption against the LLM\n",
    "    response = prompt_gpt(caption, frame_content)\n",
    "    \n",
    "    # Upsert the new captions\n",
    "    for r, t in zip(\n",
    "        rs, [response.rephrase_1, response.rephrase_2, response.rephrase_3]\n",
    "    ):\n",
    "        # Overwrite any existing re-captions\n",
    "        existing_instances = label_row.get_classification_instances(filter_ontology_classification=r)\n",
    "        for existing_instance in existing_instances:\n",
    "            label_row.remove_classification(existing_instance)\n",
    "\n",
    "        # Create new instances\n",
    "        ins = r.create_instance()\n",
    "        ins.set_answer(t, attribute=r.attributes[0])\n",
    "        ins.set_for_frames(0)\n",
    "        label_row.add_classification_instance(ins)\n",
    "\n",
    "    # Save the label row\n",
    "    label_row.save()\n",
    "    \n",
    "    # Return the pathway name to follow\n",
    "    return \"Completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Runner\n",
    "\n",
    "Now let's create and configure the Runner with our task agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Runner with your project hash\n",
    "runner = Runner(project_hash=\"your-project-hash-here\")\n",
    "\n",
    "# Register the recaption_agent with a specific stage in your workflow\n",
    "@runner.stage(\"Recaption Video\")\n",
    "def recaption_task(\n",
    "    project: Project,\n",
    "    label_row: Annotated[LabelRowV2, Annotated],\n",
    "    task: AgentTask,\n",
    "    frame_content: Annotated[np.ndarray, Annotated[dep_single_frame, {\"frame\": 0}]]\n",
    ") -> str:\n",
    "    return recaption_agent(project, label_row, task, frame_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "Now we can run our agent on the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to refresh every 60 seconds to continuously check for new tasks\n",
    "runner(refresh_every=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Workflow Requirements\n",
    "\n",
    "For this notebook to work, your project should have:\n",
    "\n",
    "1. An ontology with four text classifications:\n",
    "   - A text classification for a human-created caption\n",
    "   - Three text classifications for the LLM to fill in\n",
    "\n",
    "2. A workflow with at least one agent stage named \"Recaption Video\" and at least two pathways:\n",
    "   - \"Completed\" - for tasks that successfully generated captions\n",
    "   - \"No Human Caption\" - for tasks that had no human caption to work with\n",
    "\n",
    "### Process Flow\n",
    "\n",
    "1. The Runner periodically checks for tasks in the \"Recaption Video\" stage\n",
    "2. For each task, it loads the label row and checks if there's a human-created caption\n",
    "3. If a caption exists, it sends the caption and the first frame to GPT-4o-mini\n",
    "4. The LLM generates three alternative captions\n",
    "5. The agent adds these captions to the label row and saves it\n",
    "6. The task moves to the \"Completed\" pathway\n",
    "\n",
    "### Agent Customization\n",
    "\n",
    "You can customize this agent by:\n",
    "- Changing the system prompt to get different types of rephrases\n",
    "- Using a different LLM model\n",
    "- Adjusting the temperature for more or less creative outputs\n",
    "- Adding more classification fields for additional types of captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we created a task agent that automatically generates alternative captions for videos based on human-written descriptions. The agent uses GPT-4o-mini to create diverse, natural, and concise rephrases of the original caption.\n",
    "\n",
    "This approach can be extended to other use cases, such as:\n",
    "- Generating descriptions in multiple languages\n",
    "- Creating captions optimized for different purposes (marketing, accessibility, technical documentation)\n",
    "- Building a dataset of varied descriptions for training computer vision models\n",
    "\n",
    "By using the Encord Agent Task Runner, we've created a workflow that can process tasks in batches, automatically retry failed operations, and continuously monitor for new tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
